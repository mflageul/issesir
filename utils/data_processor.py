import pandas as pd
import numpy as np
from pathlib import Path
import os

class DataProcessor:
    def __init__(self):
        self.objectifs = {
            'closure_rate': 13.0,
            'satisfaction_q1': 92.0,
            'boutique_response': 30.0,
            'never_response': 70.0
        }

    def load_and_merge_data(self, files, enable_fiabilization=False):
        """Load and merge all Excel files with improved format handling"""
        print("=== CHARGEMENT DES DONN√âES ===")
        
        try:
            # Load Excel files with engine detection and fallback options
            def safe_read_excel(file_path, sheet_name=0):
                """Safely read Excel file with enhanced validation and multiple engines"""
                print(f"üîÑ Lecture fichier: {os.path.basename(file_path)}")
                
                # First check if file exists and basic validation
                if not os.path.exists(file_path):
                    raise Exception(f"Fichier non trouv√©: {file_path}")
                
                file_size = os.path.getsize(file_path)
                print(f"üìè Taille fichier: {file_size} bytes")
                
                if file_size == 0:
                    raise Exception(f"Fichier vide: {os.path.basename(file_path)}")
                
                if file_size < 100:  # Fichier trop petit pour √™tre un Excel valide
                    raise Exception(f"Fichier trop petit pour √™tre un Excel valide: {os.path.basename(file_path)} ({file_size} bytes)")
                
                # Check file signature to validate it's actually an Excel file
                try:
                    with open(file_path, 'rb') as f:
                        header = f.read(8)
                        # Check for common Excel signatures
                        is_xlsx = header.startswith(b'PK\x03\x04')  # ZIP signature for .xlsx
                        is_xls = header.startswith(b'\xd0\xcf\x11\xe0')  # OLE2 signature for .xls
                        
                        if not (is_xlsx or is_xls):
                            print(f"‚ö†Ô∏è Signature fichier non reconnue: {header.hex()}")
                            print(f"‚ö†Ô∏è Le fichier ne semble pas √™tre un vrai fichier Excel")
                except Exception as header_err:
                    print(f"‚ö†Ô∏è Impossible de v√©rifier l'en-t√™te: {header_err}")
                
                # Try with openpyxl first (for .xlsx files)
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')
                    print(f"‚úÖ Succ√®s avec openpyxl: {len(df)} lignes")
                    return df
                except Exception as e1:
                    print(f"‚ö†Ô∏è √âchec openpyxl: {str(e1)[:100]}...")
                    
                    # Try with xlrd (for .xls files)
                    try:
                        df = pd.read_excel(file_path, sheet_name=sheet_name, engine='xlrd')
                        print(f"‚úÖ Succ√®s avec xlrd: {len(df)} lignes")
                        return df
                    except Exception as e2:
                        print(f"‚ö†Ô∏è √âchec xlrd: {str(e2)[:100]}...")
                        
                        # Try without specifying engine (pandas auto-detection)
                        try:
                            df = pd.read_excel(file_path, sheet_name=sheet_name)
                            print(f"‚úÖ Succ√®s avec auto-d√©tection: {len(df)} lignes")
                            return df
                        except Exception as e3:
                            print(f"‚ö†Ô∏è Tous les moteurs Excel ont √©chou√©")
                            print(f"‚ùå ERREUR CRITIQUE: Le fichier {os.path.basename(file_path)} est corrompu ou dans un format non support√©")
                            print(f"üí° SOLUTION: Ouvrez le fichier dans Excel et resauvegardez-le au format .xlsx")
                            print(f"üí° V√âRIFICATION: Assurez-vous que le fichier n'est pas endommag√©")
                            
                            raise Exception(f"FICHIER EXCEL CORROMPU: {os.path.basename(file_path)}. "
                                            f"Le fichier est endommag√© ou dans un format non standard. "
                                            f"Veuillez l'ouvrir dans Excel et le resauvegarder au format .xlsx. "
                                            f"Erreurs techniques: openpyxl={str(e1)[:50]}, xlrd={str(e2)[:50]}, auto={str(e3)[:50]}")
            
            # Load all Excel files with safe reading
            df_enq = safe_read_excel(files['enq_file'])
            df_case = safe_read_excel(files['case_file'])
            df_ref = safe_read_excel(files['ref_file'])
            df_acct = safe_read_excel(files['acct_file'])
            
            print(f"‚úì Enqu√™tes: {len(df_enq)} lignes")
            print(f"‚úì Tickets: {len(df_case)} lignes")
            print(f"‚úì R√©f√©rence: {len(df_ref)} lignes")
            print(f"‚úì Comptes: {len(df_acct)} lignes")
            
            # Rename columns to avoid conflicts
            df_enq = df_enq.rename(columns={'Cr√©√© par': 'Cr√©√© par enqu√™te'})
            df_case = df_case.rename(columns={'Cr√©√© par': 'Cr√©√© par ticket'})
            
            # Ensure Code compte is numeric in both datasets
            df_case['Code compte'] = pd.to_numeric(df_case['Code compte'], errors='coerce')
            df_acct['Code compte'] = pd.to_numeric(df_acct['Code compte'], errors='coerce')
            
            # Merge enquetes with tickets
            df_merged = df_enq.merge(
                df_case[['Num√©ro', 'Cr√©√© par ticket', 'Compte', 'Code compte', 'Description courte', 'Cr√©√© le', 'Mise √† jour', 'Clos par']],
                left_on='Dossier Rcbt',
                right_on='Num√©ro',
                how='left',
                suffixes=('_enq', '_ticket')
            )
            
            # Ensure Code compte is numeric in merged data
            df_merged['Code compte'] = pd.to_numeric(df_merged['Code compte'], errors='coerce')
            
            # Add site mapping
            if 'Login' in df_ref.columns and 'Site' in df_ref.columns:
                site_mapping = dict(zip(df_ref['Login'], df_ref['Site']))
                df_merged['Site'] = df_merged['Cr√©√© par ticket'].map(site_mapping)
                df_merged['Site'] = df_merged['Site'].fillna('Autres')
            else:
                df_merged['Site'] = 'Autres'
            
            # Classify boutiques
            def classify_boutique(code):
                if pd.isna(code):
                    return 'Autres'
                code_str = str(int(code)) if isinstance(code, (int, float)) else str(code)
                prefix = code_str[:3].zfill(3)
                if prefix == '039':
                    return 'Si√®ge'
                elif prefix in ['400', '499', '993']:
                    return prefix
                else:
                    return 'Mini-enseigne'
            
            df_merged['Boutique_categorie'] = df_merged['Code compte'].apply(classify_boutique)
            df_case['Boutique_categorie'] = df_case['Code compte'].apply(classify_boutique)
            df_acct['Categorie'] = df_acct['Code compte'].apply(classify_boutique)
            
            print(f"‚úì Fusion r√©ussie: {len(df_merged)} lignes")
            
            # Fiabilisation optionnelle des tickets
            if enable_fiabilization:
                print("üîß FIABILISATION ACTIV√âE")
                from .ticket_fiabilization import TicketFiabilizer
                
                fiabilizer = TicketFiabilizer(df_case, df_ref)
                
                # Analyser le potentiel de fiabilisation
                analysis = fiabilizer.analyze_fiabilization_potential()
                print(f"üìä Potentiel fiabilisation: {analysis['fiabilisation_possible']} tickets")
                
                # Fiabiliser si possible
                if analysis['fiabilisation_possible'] > 0:
                    df_case_fiabilise = fiabilizer.fiabilize_tickets()
                    
                    # Recalculer les donn√©es merged avec les tickets fiabilis√©s
                    df_merged_fiabilise = self._merge_data_sources(df_enq, df_case_fiabilise, df_ref, df_acct)
                    
                    # Statistiques finales
                    stats = fiabilizer.get_fiabilization_stats(df_case_fiabilise)
                    print(f"‚úÖ Fiabilisation termin√©e: {stats['tickets_fiabilises']} tickets fiabilis√©s")
                    
                    return {
                        'merged': df_merged_fiabilise,
                        'case': df_case_fiabilise,
                        'accounts': df_acct,
                        'ref': df_ref,
                        'fiabilization_stats': stats
                    }
            
            # Calculer les m√©triques page 2 pour r√©cup√©rer les incoh√©rences
            page2_metrics = self.calculate_page2_enhanced_metrics({
                'merged': df_merged,
                'case': df_case,
                'accounts': df_acct,
                'reference': df_ref
            })
            
            # S'assurer que toutes les cl√©s sont pr√©sentes dans le r√©sultat
            result = {
                'merged': df_merged,
                'case': df_case,
                'inconsistencies': page2_metrics.get('inconsistencies', [])
            }
            
            # Ajouter accounts et ref s'ils sont valides
            if df_acct is not None and not df_acct.empty:
                result['accounts'] = df_acct
            if df_ref is not None and not df_ref.empty:
                result['ref'] = df_ref
                
            return result
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {e}")
            raise

    def calculate_page1_metrics(self, data):
        """Calculate Page 1 synthesis metrics"""
        print("=== CALCUL PAGE 1 - SYNTH√àSE ===")
        
        df_merged = data['merged']
        df_case = data['case']
        
        # CORRECTION: Basic metrics bas√©s sur le fichier sn_customerservice_case
        total_tickets = int(df_case['Num√©ro'].nunique())
        
        # CORRECTION: tickets_boutiques = nombre total tickets sans doublons du fichier case
        # Pour rapports globaux: tous les tickets du fichier case
        tickets_boutiques = int(df_case['Num√©ro'].nunique())  # Nombre total sans doublons
        
        # System tickets
        mask_system = df_case['Clos par'].isna() | (df_case['Clos par'] == 'system')
        tickets_system = int(df_case.loc[mask_system, 'Num√©ro'].nunique())
        
        # CORRECTION: Taux de cl√¥ture = enqu√™tes r√©pondues / tickets total
        enquetes_repondues = int(df_merged['Dossier Rcbt'].nunique())
        taux_closure = round(enquetes_repondues / tickets_boutiques * 100, 1) if tickets_boutiques > 0 else 0.0
        
        # Satisfaction metrics
        q1_data = df_merged[df_merged['Mesure'].str.contains('Q1', na=False)]
        q1_unique = q1_data.drop_duplicates('Dossier Rcbt')
        satisfaits = int(q1_unique['Valeur de cha√Æne'].isin(['Tr√®s satisfaisant', 'Satisfaisant']).sum())
        total_q1 = int(len(q1_unique))
        taux_sat = round(satisfaits / total_q1 * 100, 1) if total_q1 > 0 else 0.0
        
        # CORRECTION CRITIQUE: Ajouter nb_reponses_q1 pour historique
        result = {
            'total_tickets': total_tickets,
            'tickets_boutiques': tickets_boutiques,
            'tickets_system': tickets_system,
            'nb_reponses_q1': total_q1,  # AJOUT MANQUANT pour historique correct
            'taux_closure': float(taux_closure),
            'satisfaits': satisfaits,
            'insatisfaits': total_q1 - satisfaits,
            'taux_sat': float(taux_sat),
            'closure_ok': bool(taux_closure >= self.objectifs['closure_rate']),
            'sat_ok': bool(taux_sat >= self.objectifs['satisfaction_q1'])
        }
        
        print(f"‚úì Taux de cl√¥ture: {taux_closure}% ({'‚úÖ' if result['closure_ok'] else '‚ö†Ô∏è'})")
        print(f"‚úì Taux satisfaction Q1: {taux_sat}% ({'‚úÖ' if result['sat_ok'] else '‚ö†Ô∏è'})")
        
        return result

    def calculate_page2_enhanced_metrics(self, data):
        """Calculate enhanced Page 2 metrics with detailed analysis"""
        print("=== CALCUL PAGE 2 AM√âLIOR√â - COMMENTAIRES ===")
        
        df_merged = data['merged']
        
        # Q1 data with comments (for page 2)
        q1_data_with_comments = df_merged[
            df_merged['Mesure'].str.contains('Q1', na=False) &
            df_merged['Commentaire'].notna() & 
            (df_merged['Commentaire'].str.strip() != '')
        ]
        
        # CORRECTION: All Q1 responses for percentage calculation - utiliser TOUTES les donn√©es Q1
        q1_all = df_merged[df_merged['Mesure'].str.contains('Q1', na=False)]
        # On garde la m√™me logique que calculate_basic_metrics pour la coh√©rence
        q1_unique = q1_all.drop_duplicates('Dossier Rcbt')
        total_q1_responses = int(len(q1_unique))
        total_with_comments = int(len(q1_data_with_comments.drop_duplicates('Dossier Rcbt')))
        
        # Comments percentage
        comments_percentage = round(total_with_comments / total_q1_responses * 100, 1) if total_q1_responses > 0 else 0.0
        
        # Detailed analysis by collaborator
        collaborator_analysis = self._analyze_by_collaborator(q1_data_with_comments)
        
        # Site analysis
        site_analysis = self._analyze_by_site(q1_data_with_comments)
        
        # Satisfaction distribution in comments
        satisfaction_distribution = self._analyze_satisfaction_distribution(q1_data_with_comments)
        
        # Detect inconsistencies (positive words in negative ratings)
        inconsistencies = self._detect_inconsistencies(q1_data_with_comments)
        
        result = {
            'total_q1_responses': total_q1_responses,
            'total_with_comments': total_with_comments,
            'comments_percentage': float(comments_percentage),
            'total_collaborators': int(len(collaborator_analysis)),
            'collaborator_analysis': collaborator_analysis,
            'site_analysis': site_analysis,
            'satisfaction_distribution': satisfaction_distribution,
            'inconsistencies': inconsistencies
        }
        
        print(f"‚úì Pourcentage enqu√™tes avec commentaires: {comments_percentage}%")
        print(f"‚úì Collaborateurs analys√©s: {len(collaborator_analysis)}")
        print(f"‚úì Sites analys√©s: {len(site_analysis)}")
        print(f"‚úì Incoh√©rences d√©tect√©es: {len(inconsistencies)}")
        
        return result

    def _analyze_by_collaborator(self, df_comments):
        """Analyze comments by collaborator"""
        if df_comments.empty:
            return []
        
        # Group by collaborator
        collaborator_stats = []
        
        for collaborator, group in df_comments.groupby('Cr√©√© par ticket'):
            if pd.isna(collaborator):
                continue
                
            total_comments = int(len(group))
            
            # Satisfaction breakdown
            satisfait_count = int(group['Valeur de cha√Æne'].isin(['Tr√®s satisfaisant', 'Satisfaisant']).sum())
            insatisfait_count = int(group['Valeur de cha√Æne'].isin(['Peu satisfaisant', 'Tr√®s peu satisfaisant']).sum())
            
            # Percentages
            satisfait_pct = round(satisfait_count / total_comments * 100, 1) if total_comments > 0 else 0.0
            insatisfait_pct = round(insatisfait_count / total_comments * 100, 1) if total_comments > 0 else 0.0
            
            # Site information
            site = group['Site'].iloc[0] if 'Site' in group.columns else 'Non d√©fini'
            
            collaborator_stats.append({
                'collaborator': str(collaborator),
                'site': str(site),
                'total_comments': total_comments,
                'satisfait_count': satisfait_count,
                'insatisfait_count': insatisfait_count,
                'satisfait_pct': float(satisfait_pct),
                'insatisfait_pct': float(insatisfait_pct)
            })
        
        # Sort by total comments descending
        return sorted(collaborator_stats, key=lambda x: x['total_comments'], reverse=True)

    def _analyze_by_site(self, df_comments):
        """Analyze comments by site"""
        if df_comments.empty:
            return []
        
        site_stats = []
        
        for site, group in df_comments.groupby('Site'):
            if pd.isna(site):
                continue
                
            total_comments = int(len(group))
            unique_collaborators = int(group['Cr√©√© par ticket'].nunique())
            
            # Satisfaction breakdown
            satisfait_count = int(group['Valeur de cha√Æne'].isin(['Tr√®s satisfaisant', 'Satisfaisant']).sum())
            insatisfait_count = int(group['Valeur de cha√Æne'].isin(['Peu satisfaisant', 'Tr√®s peu satisfaisant']).sum())
            
            # Percentages
            satisfait_pct = round(satisfait_count / total_comments * 100, 1) if total_comments > 0 else 0.0
            
            site_stats.append({
                'site': str(site),
                'total_comments': total_comments,
                'unique_collaborators': unique_collaborators,
                'satisfait_count': satisfait_count,
                'insatisfait_count': insatisfait_count,
                'satisfait_pct': float(satisfait_pct)
            })
        
        # Sort by total comments descending
        return sorted(site_stats, key=lambda x: x['total_comments'], reverse=True)

    def _analyze_satisfaction_distribution(self, df_comments):
        """CORRECTION MAJEURE: Analyze satisfaction distribution - HARMONIS√â AVEC TABLEAU"""
        if df_comments.empty:
            return {}
        
        # CORRECTION INCOH√âRENCE: Utiliser les m√™mes donn√©es que le tableau (unique par dossier)
        df_satisfaction = df_comments[df_comments['Valeur de cha√Æne'].notna()]
        
        if df_satisfaction.empty:
            return {}
        
        # CORRECTION CRITQUE: Prendre donn√©es uniques par Dossier RCBT comme le tableau
        df_unique = df_satisfaction.drop_duplicates('Dossier Rcbt')
        
        # Count by satisfaction level sur donn√©es harmonis√©es
        satisfaction_counts = df_unique['Valeur de cha√Æne'].value_counts()
        total = int(len(df_unique))  # CORRECTION: Total = entr√©es uniques par dossier
        
        print(f"üîç SATISFACTION DISTRIBUTION HARMONIS√âE:")
        print(f"   Total UNIQUE par dossier: {total}")
        
        distribution = {}
        for satisfaction, count in satisfaction_counts.items():
            count = int(count)
            percentage = round((count / total) * 100, 1) if total > 0 else 0.0
            print(f"   {satisfaction}: {count}/{total} = {percentage}%")
            distribution[str(satisfaction)] = {
                'count': count,
                'percentage': percentage
            }
        
        return distribution

    def _detect_inconsistencies(self, df_comments):
        """Detect inconsistencies between ratings and comments - bidirectional"""
        if df_comments.empty:
            return []
        
        # MOTS POSITIFS (forts et contextuels)
        mots_positifs = ['merci', 'parfait', 'top', 'efficace', 'rapide', 'solution', 
                        'clair', 'pr√©cis', 'excellent', 'super', 'formidable', 'g√©nial',
                        'impeccable', 'extraordinaire', 'fantastique', 'remarquable',
                        'bien accueilli', 'bien acceuilli', 'professionnel', 'comp√©tent', 'aimable', 'courtois']
        
        # MOTS N√âGATIFS (forts et contextuels) - √âTENDUS POUR MITIG√â
        # ATTENTION: "probl√®me/probleme" sera trait√© contextuellement ci-dessous
        mots_negatifs = ['catastrophique', 'horrible', 'nul', 'inadmissible', 'scandaleux',
                        'inacceptable', 'd√©cevant', 'frustrant', 'agac√©', '√©nerv√©', 'furieux',
                        'incomp√©tent', 'd√©plorable', 'lamentable', 'd√©sastreux', 'd√©√ßu', 'd√©√ßu par',
                        'insatisfait', 'm√©content', 'erreur', 'mauvais', 'qualit√© du traitement',
                        'inefficace', 'lent', 'pas bien', 'pas bon', 'pas r√©solu', 'non r√©solu',
                        'pas satisfait', 'pas content', 'rat√©', '√©chec', 'faute', 'manque',
                        'insuffisant', 'incomplet', 'inad√©quat', 'attendais mieux', 'j\'attendais mieux',
                        'attendre mieux', 'pas parfait', 'n\'√©tait pas parfait', 'ce n\'√©tait pas parfait',
                        'bloquant', 'perdre', 'business', 'fait perdre', 'perte']
        
        # D√âTECTION CONTEXTUELLE DE "PROBL√àME/PROBLEME"
        def is_probleme_negative_context(comment_text):
            """D√©termine si 'probl√®me' est dans un contexte n√©gatif ou positif"""
            comment_lower = comment_text.lower()
            
            # Contextes POSITIFS (probl√®me r√©solu/trait√© positivement)
            positive_patterns = [
                r'r[√©e]solu.{0,10}probl[e√®]me?s?', r'probl[e√®]me?s?.{0,10}r[√©e]solu',
                r'r[√©e]gl[√©e].{0,10}probl[e√®]me?s?', r'probl[e√®]me?s?.{0,10}r[√©e]gl[√©e]', 
                r'solutionn.{0,10}probl[e√®]me?s?', r'probl[e√®]me?s?.{0,10}solutionn',
                r'solution.{0,10}probl[e√®]me?s?', r'probl[e√®]me?s?.{0,10}solution',
                r'pas de probl[e√®]me?s?', r'aucun probl[e√®]me?s?', r'zero probl[e√®]me?s?',
                r'sans probl[e√®]me?s?', r'r[√©e]solution.{0,15}probl[e√®]me?s?',
                r'a\s+r[√©e]solu\s+.{0,15}probl[e√®]me?s?', r'ont\s+r[√©e]solu\s+.{0,15}probl[e√®]me?s?'
            ]
            
            # Contextes N√âGATIFS (probl√®me persistant/non r√©solu)  
            negative_patterns = [
                r'gros probl[e√®]me', r'grave probl[e√®]me', r's√©rieux probl[e√®]me',
                r'probl[e√®]me bloquant', r'probl[e√®]me majeur', r'probl[e√®]me important',
                r'probl[e√®]me non r√©solu', r'probl[e√®]me pas r√©solu'
            ]
            
            import re
            
            # V√©rifier contextes positifs (prioritaire)
            for pattern in positive_patterns:
                if re.search(pattern, comment_lower):
                    return False  # Pas n√©gatif, contexte positif
                    
            # V√©rifier contextes n√©gatifs explicites
            for pattern in negative_patterns:
                if re.search(pattern, comment_lower):
                    return True  # N√©gatif explicite
                    
            # Par d√©faut, si "probl√®me" seul ‚Üí consid√©rer comme n√©gatif
            if re.search(r'\bprobl[e√®]me?s?\b', comment_lower):
                return True
                
            return False
        
        # MOTS N√âGATIFS CONTEXTUELS (n√©cessitent une v√©rification du contexte)
        mots_negatifs_contextuels = {
            'long': ['trop long', 'tr√®s long', 'delai long', 'd√©lai long', 'temps long', 'attente long']
        }
        
        # PATTERNS TEMPORELS N√âGATIFS (d√©tection d'attente excessive)
        import re
        patterns_temporels_negatifs = [
            r'\d+\s*mn?\s+d[\'e\s]*attente',     # "18 mn d attente", "15 min d'attente" 
            r'\d+\s*mn?\s+d[\'e\s]*echange',     # "18 mn d echange", "20 min d'√©change"
            r'\d+\s*minutes?\s+pour',            # "15 minutes pour"
            r'\d+\s*mn?\s+avant',                # "10 mn avant"
            r'attendre?\s+\d+\s*mn?',            # "attendre 15 mn"
            r'patiente[rz]?\s+\d+\s*mn?'         # "patienter 20 mn"
        ]
        
        # MOTS DE CONTRASTE (d√©tectent les nuances/ambigu√Øt√©s) - √âTENDUS
        mots_contraste = ['mais', 'cependant', 'toutefois', 'n√©anmoins', 'par contre', 
                         'en revanche', 'malheureusement', 'dommage', 'regret',
                         'm√™me si', 'bien que', 'malgr√©', 'pourtant',
                         'seulement', 'uniquement', 'juste', 'sauf que']
        
        # MOTS DE CONTRASTE SP√âCIAUX (n√©cessitent des limites de mots)
        mots_contraste_speciaux = [' or ', 'or,', 'or.', 'or!', 'or?', 'or;']
        
        inconsistencies = []
        
        # TYPE 1: Notes n√©gatives avec commentaires positifs
        negative_ratings = df_comments[
            df_comments['Valeur de cha√Æne'].isin(['Peu satisfaisant', 'Tr√®s peu satisfaisant'])
        ]
        
        for _, row in negative_ratings.iterrows():
            comment = str(row['Commentaire']).lower()
            positive_words_found = [word for word in mots_positifs if word in comment]
            
            # FIABILISATION: Au moins 2 mots positifs ou 1 mot tr√®s fort
            mots_forts = ['parfait', 'excellent', 'formidable', 'impeccable', 'extraordinaire']
            mots_forts_trouves = [word for word in mots_forts if word in comment]
            
            if len(positive_words_found) >= 2 or len(mots_forts_trouves) >= 1:
                # CORRECTION PROBL√àME 1: Calculer la suggestion et v√©rifier si elle diff√®re
                original_rating = str(row['Valeur de cha√Æne'])
                suggested_rating = self._get_suggested_rating(original_rating, 'Note n√©gative / Commentaire positif')
                
                # FILTRAGE INTELLIGENT: Ne remonter que si suggestion diff√©rente de l'original
                print(f"üîç TYPE 1 - Dossier {row['Dossier Rcbt']}: '{original_rating}' ‚Üí '{suggested_rating}' ({'FILTR√â' if original_rating == suggested_rating else 'REMONT√â'})")
                if original_rating != suggested_rating:
                    inconsistencies.append({
                        'dossier': str(row['Dossier Rcbt']),
                        'collaborator': str(row['Cr√©√© par ticket']),
                        'rating': original_rating,
                        'comment': str(row['Commentaire']),
                        'inconsistency_type': 'Note n√©gative / Commentaire positif',
                        'detected_words': positive_words_found,
                        'suggested_rating': suggested_rating,
                        'impact': 'Ticket class√© comme incoh√©rent - V√©rification recommand√©e'
                    })
        
        # TYPE 2: Notes positives avec commentaires n√©gatifs  
        positive_ratings = df_comments[
            df_comments['Valeur de cha√Æne'].isin(['Tr√®s satisfaisant', 'Satisfaisant'])
        ]
        
        for _, row in positive_ratings.iterrows():
            comment = str(row['Commentaire']).lower()
            comment_original = str(row['Commentaire'])
            negative_words_found = [word for word in mots_negatifs if word in comment]
            contraste_words_found = [word for word in mots_contraste if word in comment]
            
            # D√âTECTION CONTEXTUELLE DE "PROBL√àME/PROBLEME"
            if is_probleme_negative_context(comment_original):
                if "probleme_negatif" not in negative_words_found:
                    negative_words_found.append("probleme_negatif")
            
            # CORRECTION: V√©rifier les mots de contraste sp√©ciaux avec limites
            contraste_speciaux_found = [word.strip() for word in mots_contraste_speciaux if word in comment]
            contraste_words_found.extend(contraste_speciaux_found)
            
            # V√âRIFICATION CONTEXTUELLE pour les mots n√©gatifs contextuels
            for word, contexts in mots_negatifs_contextuels.items():
                if any(context in comment for context in contexts):
                    if word not in negative_words_found:
                        negative_words_found.append(word)
            
            # V√âRIFICATION PATTERNS TEMPORELS N√âGATIFS (attente excessive)
            for pattern in patterns_temporels_negatifs:
                if re.search(pattern, comment):
                    if "delai_excessif" not in negative_words_found:
                        negative_words_found.append("delai_excessif")
            
            # FIABILISATION: Au moins 1 mot n√©gatif fort OU mot de contraste avec positif+n√©gatif
            detected_words = negative_words_found + contraste_words_found
            
            # Cas 1: Mots n√©gatifs directs
            # Cas 2: Commentaires mitig√©s (positif + contraste + n√©gatif)
            # Cas 3: NOUVEAU - Commentaires nuanc√©s avec contraste m√™me sans mots n√©gatifs forts
            has_positive = any(word in comment for word in mots_positifs)
            has_contraste = len(contraste_words_found) >= 1
            has_negative = len(negative_words_found) >= 1
            
            # AM√âLIORATION SP√âCIALE: D√©tecter expressions nuanc√©es
            has_nuanced_negative = any(phrase in comment for phrase in [
                'attendais mieux', 'j\'attendais mieux', 'attendre mieux',
                'pas parfait', 'n\'√©tait pas parfait', 'ce n\'√©tait pas parfait'
            ])
            
            # LOGIQUE AM√âLIOR√âE: D√©tecter "bien accueilli mais d√©√ßu" et variations
            # Inclure les expressions nuanc√©es dans les mots d√©tect√©s
            nuanced_words_found = [phrase for phrase in [
                'attendais mieux', 'j\'attendais mieux', 'attendre mieux',
                'pas parfait', 'n\'√©tait pas parfait', 'ce n\'√©tait pas parfait'
            ] if phrase in comment]
            
            if has_nuanced_negative:
                detected_words.extend(nuanced_words_found)
            
            if has_negative or (has_positive and has_contraste) or (has_contraste and has_nuanced_negative):
                inconsistency_type = 'Note positive / Commentaire mitig√©' if has_contraste else 'Note positive / Commentaire n√©gatif'
                
                # CORRECTION PROBL√àME 1: Calculer la suggestion et v√©rifier si elle diff√®re
                original_rating = str(row['Valeur de cha√Æne'])
                suggested_rating = self._get_suggested_rating(original_rating, inconsistency_type)
                
                # FILTRAGE INTELLIGENT: Ne remonter que si suggestion diff√©rente de l'original
                print(f"üîç TYPE 2 - Dossier {row['Dossier Rcbt']}: '{original_rating}' ‚Üí '{suggested_rating}' ({'FILTR√â' if original_rating == suggested_rating else 'REMONT√â'})")
                if original_rating != suggested_rating:
                    inconsistencies.append({
                        'dossier': str(row['Dossier Rcbt']),
                        'collaborator': str(row['Cr√©√© par ticket']),
                        'rating': original_rating,
                        'comment': str(row['Commentaire']),
                        'inconsistency_type': inconsistency_type,
                        'detected_words': detected_words,
                        'suggested_rating': suggested_rating,
                        'impact': 'Ticket class√© comme incoh√©rent - V√©rification recommand√©e'
                    })
        
        return inconsistencies
    
    def _get_suggested_rating(self, original_rating: str, inconsistency_type: str) -> str:
        """CORRECTION PROBL√àME 1: Sugg√®re une correction bas√©e sur le type d'incoh√©rence"""
        if inconsistency_type == 'Note n√©gative / Commentaire positif':
            if original_rating in ['Tr√®s peu satisfaisant', 'Peu satisfaisant']:
                return 'Satisfaisant'
        elif inconsistency_type in ['Note positive / Commentaire n√©gatif', 'Note positive / Commentaire mitig√©']:
            # LOGIQUE AJUST√âE: Pour commentaires mitig√©s, sugg√©rer une diminution appropri√©e
            if inconsistency_type == 'Note positive / Commentaire mitig√©':
                # Pour les commentaires mitig√©s (ex: "probl√®me mais solution"), diminution mod√©r√©e
                if original_rating == 'Tr√®s satisfaisant':
                    return 'Satisfaisant'  # Diminution d'un niveau pour nuance
                elif original_rating == 'Satisfaisant':
                    return 'Peu satisfaisant'
            elif original_rating in ['Tr√®s satisfaisant', 'Satisfaisant']:
                # Pour commentaires purement n√©gatifs, diminution plus forte
                return 'Peu satisfaisant'
        return original_rating
    
    def get_page2_data(self, data):
        """Get detailed data for page 2 (comments)"""
        df_merged = data['merged']
        
        # Q1 data with comments
        q1_data_with_comments = df_merged[
            df_merged['Mesure'].str.contains('Q1', na=False) &
            df_merged['Commentaire'].notna() & 
            (df_merged['Commentaire'].str.strip() != '')
        ]
        
        return q1_data_with_comments
    
    def get_page3_data(self, data):
        """Get data for page 3 (unsatisfied without comments)"""
        df_merged = data['merged']
        
        # Clients insatisfaits sans commentaire
        insatisfied_no_comment = df_merged[
            df_merged['Mesure'].str.contains('Q1', na=False) &
            df_merged['Valeur de cha√Æne'].isin(['Peu satisfaisant', 'Tr√®s peu satisfaisant']) &
            (df_merged['Commentaire'].isna() | (df_merged['Commentaire'].str.strip() == ''))
        ]
        
        return insatisfied_no_comment
